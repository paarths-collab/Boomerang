import streamlit as st
import sys
import pathlib
import json

# --- Path setup ---
# This ensures the page can find the 'agents' directory
sys.path.append(str(pathlib.Path(__file__).parent.parent))
from agents.orchestrator import Orchestrator

# --- Page Configuration ---
st.set_page_config(page_title="AI Consultant", layout="wide")
st.title("ðŸ¤– Your Personal AI Investment Consultant")
st.markdown("Get a personalized investment plan based on your profile, powered by local AI models.")

# --- Load the Orchestrator ---
# @st.cache_resource is essential to prevent re-initializing agents on every interaction.
@st.cache_resource
def load_orchestrator():
    config_path = "quant-company-insights-agent/config.yaml"
    print("--- LOADING ORCHESTRATOR FOR AI CONSULTANT PAGE ---")
    return Orchestrator.from_file(config_path)

orchestrator = load_orchestrator()

# --- User Profile Input ---
with st.sidebar:
    st.header("ðŸ‘¤ Your Profile")
    
    profession = st.selectbox(
        "Your Profession", 
        ["Student", "Salaried Professional", "Business Owner"]
    )
    goal = st.selectbox(
        "Your Primary Goal", 
        ["Maximum Savings (Long-Term Growth)", "Steady Income (Dividends)", "Funding a Startup (Aggressive Growth)"]
    )
    risk = st.selectbox(
        "Your Risk Tolerance", 
        ["Low", "Medium", "High (Aggressive)"]
    )
    
    st.markdown("---")
    st.header("ðŸ§  Choose Your AI Consultant")
    
    # --- NEW: Model Selection ---
    # The user can now choose between your installed local models
    ai_model = st.selectbox(
        "Select AI Model",
        ["llama3", "mistral"] 
    )
    
    run_button = st.button("ðŸ’¡ Generate My Plan", use_container_width=True)

# --- Main Panel Execution ---
if run_button:
    # Check if the Ollama server is actually available
    if not orchestrator.llm_agent.ollama_available:
        st.error("Ollama server not detected. Please ensure Ollama is running on your machine to use the AI Consultant.")
    else:
        st.header("Here is Your Personalized Investment Plan")
        st.info(f"This recommendation was generated by your local **{ai_model}** model via Ollama.")

        # 1. Assemble the user profile into a dictionary
        user_profile = {
            "profession": profession, 
            "goal": goal, 
            "risk": risk
        }
        
        with st.spinner(f"Your {ai_model} consultant is analyzing the market and tailoring a plan for you..."):
            # 2. Call the Orchestrator's recommendation method
            # We explicitly tell it to use the 'ollama' provider and the user-selected model.
            recommendation_markdown = orchestrator.get_ai_recommendation(
                user_profile=user_profile,
                provider='ollama',
                model=ai_model
            )
            
            # 3. Display the result
            st.markdown(recommendation_markdown)

else:
    st.info("Please configure your profile in the sidebar and choose your preferred AI model, then click 'Generate My Plan' to get started.")


# import streamlit as st
# import sys
# import pathlib
# import json

# # --- Path setup to allow importing from the 'agents' directory ---
# sys.path.append(str(pathlib.Path(__file__).parent.parent))
# from agents.orchestrator import Orchestrator

# # --- Page Configuration ---
# st.set_page_config(page_title="AI Consultant", layout="wide")
# st.title("ðŸ¤– Your Personal AI Investment Consultant")
# st.markdown("Get a personalized investment plan based on your profile and live market data.")

# # --- Cached Functions for Performance ---

# @st.cache_resource
# def load_orchestrator():
#     """
#     Loads the orchestrator once and caches the resource for the entire session.
#     """
#     config_path = "quant-company-insights-agent/config.yaml"
#     print("--- LOADING ORCHESTRATOR (this should only run once) ---")
#     return Orchestrator.from_file(config_path)

# @st.cache_data(ttl=600) # Cache the AI's response for 10 minutes
# def get_cached_recommendation(_orchestrator, user_profile_json: str):
#     """
#     This wrapper function allows Streamlit to cache the AI's response.
#     We pass the user_profile as a JSON string to make it hashable for the cache.
#     The underscore on _orchestrator tells Streamlit not to hash the complex object.
#     """
#     print(f"--- CACHE MISS: Calling real get_personalized_recommendation for profile: {user_profile_json} ---")
#     # Convert the JSON string back to a dictionary before passing to the orchestrator
#     user_profile = json.loads(user_profile_json)
#     return _orchestrator.get_personalized_recommendation(user_profile)

# # --- Load the Orchestrator ---
# orchestrator = load_orchestrator()

# # --- Sidebar for User Input ---
# # This section is now defined only ONCE.
# with st.sidebar:
#     st.header("ðŸ‘¤ Tell Us About Yourself")
    
#     profession = st.selectbox(
#         "Your Profession", 
#         ["Student", "Salaried Professional", "Business Owner"]
#     )
#     goal = st.selectbox(
#         "Your Primary Goal", 
#         ["Maximum Savings (Long-Term Growth)", "Steady Income (Dividends)", "Funding a Startup (Aggressive Growth)"]
#     )
#     risk = st.selectbox(
#         "Your Risk Tolerance", 
#         ["Low", "Medium", "High (Aggressive)"]
#     )
#     horizon = st.selectbox(
#         "Your Time Horizon", 
#         ["Short-Term (< 2 years)", "Medium-Term (2-5 years)", "Long-Term (5+ years)"]
#     )
    
#     # The "Generate My Plan" button is also here.
#     run_button = st.button("ðŸ’¡ Generate My Plan", use_container_width=True)


# # --- Main Panel Execution ---
# # This block runs ONLY when the button in the sidebar is clicked.
# if run_button:
#     st.header("Here is Your Personalized Investment Plan")
    
#     # 1. Assemble the user profile into a dictionary
#     user_profile = {
#         "profession": profession, 
#         "goal": goal, 
#         "risk": risk, 
#         "horizon": horizon
#     }
    
#     with st.spinner("Your AI consultant is analyzing the markets and tailoring a plan for you..."):
#         # 2. Convert profile to a JSON string to make it hashable for caching
#         user_profile_json_string = json.dumps(user_profile, sort_keys=True)
        
#         # 3. Call the cached function with the orchestrator and the hashable profile
#         recommendation_markdown = get_cached_recommendation(orchestrator, user_profile_json_string)
        
#         # 4. Display the result
#         st.markdown(recommendation_markdown)

# else:
#     st.info("Please configure your profile in the sidebar and click 'Generate My Plan' to get started.")